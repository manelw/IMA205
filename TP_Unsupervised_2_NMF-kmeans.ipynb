{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQdPnbk6EeCs"
   },
   "source": [
    "## Practical Session - Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqlB0cJvEeCu"
   },
   "source": [
    "This Practical session is about unsupervised learning. We will use the dimensionality reduction and clustering techniques presented this morning to analyze both toy examples and real images.\n",
    "\n",
    "Please answer the questions and complete the code where you see (`XXXXXXXXXX`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBkBY3QIEeCv"
   },
   "source": [
    "First let's load the functions we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7Ay0njaQEeCw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from scipy import linalg as LA\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.stats import gennorm\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "from skimage import img_as_bool\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from scipy.spatial.distance import dice\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Dimensionnality reduction: NMF\n",
    "\n",
    "## I - 1) Dataset\n",
    "\n",
    "You will first work on the same face images as during the practical work 1.\n",
    "\n",
    "Load the original images present in the files *'YaleB\\_32x32.mat'*. This is a small part of the freely available Extended Yale Face Database B downloaded from http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html. It contains 2414 cropped images resized to 32x32 pixels. Every image is represented as a vector 1x1024 and all images are stacked in a matrix called data. There are 38 subjects with around 64 near frontal images per individual under different illumination conditions. Once loaded and normalised the data, such that the pixels are between 0 and 1, you can plot images using the function *'imshow'*.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "The goal of this part is to evaluate the performance of the dimensionality reduction techniques presented this morning for face recognition. We divide the data-set into two parts, training and test. For every dimensionality reduction technique, you will first extract a set of basis images from your training data-set. Then, you will project the test subjects in this new basis and use the nearest neighbor algorithm to evaluate the performance of the dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and define a function to plot the faces. If you do not remember how the dataset is constituted, please refer to the previous practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  !pip install googledrivedownloader\n",
    "  from googledrivedownloader import download_file_from_google_drive \n",
    "  download_file_from_google_drive(file_id='1rgICXtcIAgDqSoHnNXNZMD_iNABF3RZA',dest_path='./YaleB_32x32.mat')\n",
    "else:\n",
    "  print('You are not using Colab. Please define working_dir with the absolute path to the folder where you downloaded the data')\n",
    "\n",
    "# Please modify working_dir only if you are using your Anaconda (and not Google Colab)\n",
    "Working_directory=\"./\"\n",
    "\n",
    "\n",
    "x = loadmat(Working_directory + './YaleB_32x32.mat')\n",
    "data=x['fea']\n",
    "d=data.shape[1] # number of pixels of the images\n",
    "subjectIndex=x['gnd'] # we have one index per subject\n",
    "maxValue = np.max(np.max(data)) # max intensity value\n",
    "data = data/maxValue; # Scale pixels to [0,1]\n",
    "\n",
    "Ns=len(np.unique(subjectIndex)); # Number subjects\n",
    "Is=round(len(subjectIndex)/Ns) # Number images per subject (on average, not the same number for every subject)\n",
    "r=int(np.sqrt(d)) # number rows of each image\n",
    "c=r # number columns of each image, equal to row since images are square\n",
    "\n",
    "print('There are', data.shape[0], 'facial images and each image has', d, 'pixels' )\n",
    "print('There are', Ns, 'different subjects and each subject has on average', Is, 'images')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Xtrain, Xtest, Id_Train, Id_Test = train_test_split(data,subjectIndex,test_size=0.20,stratify=subjectIndex, random_state=44)\n",
    "Xctest=Xtest-np.mean(Xtest,axis=0) # centering\n",
    "Xctrain=Xtrain-np.mean(Xtrain,axis=0) # centering\n",
    "\n",
    "\n",
    "\n",
    "def plotFaces(data,r,c,ncol=2,N=0,indeces=None,title=None):\n",
    "    # data: each face is a row in data\n",
    "    # r,c = number of rows and columns of each image\n",
    "    # n_col = number of columns for subplots\n",
    "    # N = random images to plot (used only if indeces is empty)\n",
    "    # indeces = indeces of images to plot\n",
    "    # title = title of the plot\n",
    "\n",
    "   \n",
    "    if indeces is None:\n",
    "        if N==0:\n",
    "            raise NameError('You should define either N or indeces')\n",
    "        else:\n",
    "            print('Use N random subjects')\n",
    "            indeces=np.random.randint(0,data.shape[0],(N,1))\n",
    "            \n",
    "    nrow=math.ceil(len(indeces)/ncol)\n",
    "    \n",
    "    fig=plt.figure(figsize=(17, 6))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, index in enumerate(indeces):\n",
    "        fig.add_subplot(nrow, ncol, i+1)\n",
    "        plt.imshow(np.resize(data[index,:],(r,c)).T,origin='upper',cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - 2) NMF\n",
    "\n",
    "Here you will test Non-negative Matrix factorization. The basis images of the training are in the matrix $W_{train}$ and the scores (or coefficients) to test the performance in $H_{train}$. The test scores are computed as $H_{test}=W_{train}^{-1}X_{test}$.\n",
    "\n",
    "**Question**\n",
    "\n",
    "1. Use the scikit-learn implementation to test the performance.\n",
    "2. Plot the basis images and compare them with respect to the basis images obtained using PCA and ICA in the last practical work. What can you say ?\n",
    "3. What about the performances of NMF, i.e. computational time and classification accuracy ?\n",
    "4. Do you think that it is a good idea to use a PCA before the NMF algorithm? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF (scikit-learn implementation)\n",
    "Ncomponents=100\n",
    "model = NMF(init='random', solver='mu', n_components=Ncomponents, tol=1e-3,max_iter=300, random_state=0)\n",
    "WtrainNNMF = model.fit_transform(Xtrain.T)\n",
    "HtrainNNMF = model.components_\n",
    "\n",
    "plotFaces(WtrainNNMF.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='NMF-faces') \n",
    "\n",
    "# to invert the matrix you can use the function LA.pinv\n",
    "Htest_nnmf = np.dot(LA.pinv(WtrainNNMF),Xtest.T)\n",
    "\n",
    "print('NMF uses ', Ncomponents, ' features')\n",
    "\n",
    "# Score\n",
    "NN=KNeighborsClassifier(n_neighbors=1)\n",
    "NN.fit(HtrainNNMF.T,Id_Train.ravel())\n",
    "print('Percentage of correct answer using NMF is ', NN.score(Htest_nnmf.T,Id_Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own implementation in `NNMFLecture` following the lecture slides. Complete the missing lines (`XXXXXXXXXX`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNMFLecture(X,r=None,N_Iter=1000,tolerance=1e-3,plot_evolution=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "    %           X: is a [dxN] matrix. Every column (x) is an observation and every\n",
    "    %           row consists of features.\n",
    "    %\n",
    "    %           r: size of the matrices W and H\n",
    "    %\n",
    "    %           (Optional) N_Iter: maximum number of iterations\n",
    "    %\n",
    "    %           (Optional) tolerance: convergence criteria threshold\n",
    "    %\n",
    "    %           (Optional) plot_evolution: plot evolution convergence criteria\n",
    "    %\n",
    "    % Outputs:\n",
    "    %           W: is a [d x r] matrix containing the basis images in its\n",
    "    %           columns\n",
    "    %           \n",
    "    %           H: is a [r x N] matrix containing the loadings (h) in its columns\n",
    "    %           of the linear combination: x=Wh \n",
    "    %\n",
    "  '''\n",
    "    if r is None:\n",
    "        r=X.shape[0]\n",
    "        \n",
    "    # Test for positive values\n",
    "    if np.min(X) < 0:\n",
    "        raise NameError('Input matrix X has negative values !')      \n",
    "\n",
    "    # Size\n",
    "    d,N=X.shape\n",
    "   \n",
    "    # Initialization\n",
    "    W=XXXXXXXX\n",
    "    H=XXXXXXXX   \n",
    "    \n",
    "    # parameters for convergence\n",
    "    k = 0\n",
    "    delta = np.inf\n",
    "    eps=np.finfo(float).eps\n",
    "    evolutionDelta=[]\n",
    " \n",
    "    while delta > tolerance and k < N_Iter:\n",
    "        \n",
    "        # multiplicative method      \n",
    "        \n",
    "        XH=np.dot(X,H.T)\n",
    "        HH=np.dot(H,H.T)\n",
    "        for i in range(20):\n",
    "            W = np.divide(XXXXXXXX + eps)\n",
    "                       \n",
    "        H = np.divide(XXXXXXXX + eps)\n",
    "\n",
    "        # Convergence indices\n",
    "        k = k + 1           \n",
    "        diff=X-np.dot(W,H)     \n",
    "        #delta = np.sqrt(np.sum(diff**2)) / np.sqrt(np.sum(X**2)) # |X-WH|_2 / |X|_2\n",
    "        delta = LA.norm(diff,'fro') / LA.norm(X,'fro') # sqrt(trace(diff'*diff)) / sqrt(trace(X'*X))\n",
    "        evolutionDelta.append(delta)\n",
    "        \n",
    "        if k==1 or k%100==0:\n",
    "            print('Iteration NMF number ', k, ' out of ', N_Iter , ', delta = ', delta, ', error (norm delta): ', LA.norm(diff))\n",
    "     \n",
    "    if k==N_Iter:\n",
    "        print('Maximum number of iterations reached ! delta = ', delta)\n",
    "    else:\n",
    "        print('Convergence achieved ( delta = ', delta, ') in ', k, ' iterations')\n",
    "    \n",
    "    if plot_evolution==1:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(range(k),evolutionDelta,'bx--', linewidth=4, markersize=12)  \n",
    "        plt.title('Evolution of error - NMF')\n",
    "        plt.show()\n",
    "    \n",
    "    return W,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF (your own implementation)\n",
    "Ncomponents=100\n",
    "Wtrain_nnmf,Htrain_nnmf = NNMFLecture(Xtrain.T,r=Ncomponents,N_Iter=300,tolerance=1e-3,plot_evolution=1)\n",
    "plotFaces(Wtrain_nnmf.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='NNMF-faces') \n",
    "\n",
    "# to invert the matrix you can use the function LA.pinv\n",
    "Htest_nnmf = np.dot(XXXXXXXX,Xtest.T)\n",
    "\n",
    "print('NMF uses ', Ncomponents, ' features')\n",
    "\n",
    "# Score\n",
    "NN.fit(Htrain_nnmf.T,Id_Train.ravel())\n",
    "print('Percentage of correct answer using NMF is ', NN.score(Htest_nnmf.T,Id_Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you observe?\n",
    "- What is the limitation of doing an inversion for Htest_nnmf? How would you avoid that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-3) ICA and NMF in the presence of noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare ICA and NMF results in the presence of noise within the dataset. Please note that during the class on ICA, the dataset $X$ was assumed to be noiseless: we were looking for a decomposition such that $X = AS$, and not $X = AS + N$, where $N$ would be a noise term.\n",
    "\n",
    "1. How do you think that ICA might behave in the presence of noise? You might in particular consider the case of a Gaussian noise N.\n",
    "2. On the other hand, do you think that NMF behaves better? If so, for what kind of noises?\n",
    "\n",
    "In the following, we will try to compare the behavior of ICA and NMF in the presence of noise. Do the following:\n",
    "- add some Gaussian noise to both the training and the testing dataset.\n",
    "- apply both NMF and ICA (the second architecture you implemented in the previous practical work) on the noisy dataset. For both, you can use 250 features.\n",
    "- use the NN algorithm to classify the images from the features extracted from ICA and NMF.\n",
    "\n",
    "Redo the same for different noise levels. Plot a curve of the classification accuracy when either ICA or NMF are used as pre-processing steps. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmaTab = np.array([0,0.05,0.1,0.15,0.2,0.25,0.3,0.4,0.5])\n",
    "scoreNMF = np.zeros(len(sigmaTab))\n",
    "scoreICA = np.zeros(len(sigmaTab))\n",
    "scoreICA1 = np.zeros(len(sigmaTab))\n",
    "\n",
    "for i in range(len(sigmaTab)):\n",
    "    # Data generation\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    \n",
    "    ##################\n",
    "    # NMF\n",
    "    Ncomponents=250\n",
    "    model = NMF(init='random', solver='mu', n_components=Ncomponents, tol=1e-3,max_iter=300, random_state=0)\n",
    "    WtrainNNMF = model.fit_transform(XtrainNoise.T)\n",
    "    HtrainNNMF = model.components_\n",
    "\n",
    "    # to invert the matrix you can use the function LA.pinv\n",
    "    Htest_nnmf = np.dot(XXXXXXXX,XtestNoise.T)\n",
    "\n",
    "    print('NNMF uses ', Ncomponents, ' features')\n",
    "\n",
    "    # Score\n",
    "    NN=KNeighborsClassifier(n_neighbors=1)\n",
    "    NN.fit(XXXXXXXX,Id_Train.ravel())\n",
    "    print('Percentage of correct answer using NNMF is ', NN.score(XXXXXXXX,Id_Test))\n",
    "    scoreNMF[i] = XXXXXXXX\n",
    "\n",
    "    ##################\n",
    "    ## ICA (you don't have anything to change here)\n",
    "    # Second architecture (scikit-learn implementation)\n",
    "    XctestNoise=XtestNoise-np.mean(XtestNoise,axis=0) # centering\n",
    "    XctrainNoise=XtrainNoise-np.mean(XtrainNoise,axis=0) # centering\n",
    "    \n",
    "    # Second architecture (scikit-learn implementation).\n",
    "    pca = PCA(random_state=1) \n",
    "    YpcaTrain=pca.fit_transform(XctrainNoise)\n",
    "    UpcaTrain=pca.components_.T # we want PC on columns\n",
    "    var_explained_pca=pca.explained_variance_ratio_\n",
    "\n",
    "    # We use the PCA projection to speed up results\n",
    "    PCAComp=250\n",
    "\n",
    "    # Selection of the eigenvectors \n",
    "    Yr_train_PCA=YpcaTrain[:,:PCAComp]\n",
    "    Ur_train_PCA=UpcaTrain[:,:PCAComp]\n",
    "    Yr_test_PCA=np.dot(XctestNoise,Ur_train_PCA)\n",
    "\n",
    "    ICA= FastICA(whiten='arbitrary-variance', fun='exp', max_iter=30000, tol=1e-4, algorithm='parallel', random_state=1)\n",
    "    Yica=ICA.fit_transform(Yr_train_PCA)\n",
    "    S_train_ICA=Yica.T\n",
    "    W_train_ICA=ICA.components_\n",
    "\n",
    "    ICAFAces=np.dot(Ur_train_PCA,W_train_ICA.T) \n",
    "    Y_train_ICA=S_train_ICA\n",
    "    Y_test_ICA=np.dot(W_train_ICA,Yr_test_PCA.T)\n",
    "\n",
    "    # Plot the ICA-faces\n",
    "    plotFaces(ICAFAces.T,r,c,ncol=2,indeces=np.arange(0,10,1),title='ICA-faces')      \n",
    "\n",
    "    print('ICA uses ', Y_train_ICA.shape[0], ' features')\n",
    "\n",
    "    # Score ICA\n",
    "    NN.fit(Y_train_ICA.T,Id_Train.ravel())\n",
    "    print('Percentage of correct answer using ICA is ', NN.score(XXXXXXXX,Id_Test.ravel()))\n",
    "    scoreICA[i] = NN.score(XXXXXXXX,Id_Test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(),plt.plot(sigmaTab,scoreNMF)\n",
    "plt.plot(sigmaTab,scoreICA)\n",
    "plt.legend(['NMF','ICA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Clustering: K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work on the K-means algorithm. First, the K-means algorithm will be used to cluster the toy examples datasets of the first practical work. Then, it will be used to segment real images.\n",
    "\n",
    "## II - 1) Toy examples\n",
    "\n",
    "Let us first define the toy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rmr-d5j3EeC3"
   },
   "outputs": [],
   "source": [
    "def generate_scenario(scenario=3, n_samples0 = 100, n_samples1 = 30):\n",
    "\n",
    "    y = np.concatenate((np.zeros([n_samples0,1]) , np.ones([n_samples1,1])) , axis=0)\n",
    "\n",
    "    if scenario == 1: \n",
    "        # Separate Gaussian\n",
    "        mean0 = [2, 3]\n",
    "        mean1 = [12, 14]\n",
    "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
    "        cov1 = 2 ** 2 * np.eye(2)\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "    elif scenario == 2:\n",
    "        # Overlapping Gaussian\n",
    "        mean0 = [2, 3]\n",
    "        mean1 = [5, 7]\n",
    "        cov0 = [[1, 1.5], [1.5 ,3]]\n",
    "        cov1 = [[2, 3], [3 ,6]]\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "        \n",
    "    elif scenario == 3:\n",
    "        # Overlapping Gaussian\n",
    "        mean0 = [0, 0]\n",
    "        mean1 = [0, 0]\n",
    "        cov0 = [[50, 15], [15, 6]]#cov0 = [[50, 4], [4, 2]]\n",
    "        cov1 = [[2, 0], [0 ,50]]#cov1 = [[2, 0], [0 ,50]]\n",
    "        X0 = np.random.multivariate_normal(mean0, cov0, n_samples0, check_valid='raise')\n",
    "        X1 = np.random.multivariate_normal(mean1, cov1, n_samples1, check_valid='raise')\n",
    "        \n",
    "        \n",
    "    elif scenario == 4:\n",
    "        # Circles\n",
    "        # 1 circle\n",
    "        angle0=np.linspace(0, 2 * np.pi, n_samples0);\n",
    "        X0=np.vstack((8*np.cos(angle0) , 8*np.sin(angle0))).T\n",
    "        \n",
    "        # 2 circle\n",
    "        angle1=np.linspace(0, 2 * np.pi, n_samples1);\n",
    "        X1=np.vstack((2*np.cos(angle1) , 2*np.sin(angle1))).T\n",
    "\n",
    "    return X0,X1,y\n",
    "\n",
    "def plotResults(X0=None,X1=None,y=None,U=None,Y=None,const=1,title=''):\n",
    "\n",
    "    X=np.concatenate((X0,X1),axis=0)\n",
    "    \n",
    "    N0=np.sum(y==0)\n",
    "    N1=np.sum(y==1)\n",
    "    \n",
    "    fig=plt.figure(figsize=(17, 6))\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 1)\n",
    "    plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
    "    plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
    "    if U is not None:\n",
    "        average=X.mean(axis=0)\n",
    "        sd=LA.norm(X.std(axis=0))\n",
    "        u0=U[:,0]*const*sd;\n",
    "        u1=U[:,1]*const*sd;\n",
    "        plt.plot([average[0]-u0[0], average[0]+u0[0]],[average[1]-u0[1], average[1]+u0[1]], c='g',linewidth=4, label='C 1' )\n",
    "        plt.plot([average[0]-u1[0], average[0]+u1[0]],[average[1]-u1[1], average[1]+u1[1]], c='k',linewidth=4, label='C 2' )\n",
    "        plt.title('Original data and components')\n",
    "    else:\n",
    "        plt.title('Original data')\n",
    "    plt.legend()\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 2)\n",
    "    plt.scatter(Y[np.where(y == 0)[0],0], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
    "    plt.scatter(Y[np.where(y == 1)[0],0], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
    "    ax.set_title(title + '\\n Scores on 1st component')\n",
    "    \n",
    "    ax  = fig.add_subplot(1, 3, 3)\n",
    "    plt.scatter(Y[np.where(y == 0)[0],1], np.zeros((N0,1)), c='r', s=3, marker='o', label='Class 0')\n",
    "    plt.scatter(Y[np.where(y == 1)[0],1], np.zeros((N1,1)), c='b', s=3, marker='x', label='Class 1')\n",
    "    plt.legend()\n",
    "    plt.title('Scores on 2nd component')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def frontiere(model, X, y, step=50):\n",
    "\n",
    "    labels = np.unique(y)\n",
    " \n",
    "    min_tot = np.min(X)\n",
    "    max_tot = np.max(X)\n",
    "    delta = (max_tot - min_tot) / step\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot, max_tot, delta),\n",
    "                         np.arange(min_tot, max_tot, delta))\n",
    "    z = np.array( model.predict(np.c_[xx.ravel(), yy.ravel() ]) )\n",
    "    z = z.reshape(xx.shape)\n",
    "   \n",
    "    plt.imshow(z, origin='lower', extent=[min_tot, max_tot, min_tot, max_tot],\n",
    "               interpolation=\"mitchell\", cmap='RdBu')\n",
    "    \n",
    "    cbar = plt.colorbar(ticks=labels)\n",
    "    cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
    "    plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1') \n",
    "    \n",
    "    plt.ylim([min_tot, max_tot])\n",
    "    plt.xlim([min_tot, max_tot])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNlIyjsHEeC7"
   },
   "source": [
    "Now, create the data we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "M6Rpg84vEeC9",
    "outputId": "471594bf-e79b-4599-fcc6-e24eceba64b7"
   },
   "outputs": [],
   "source": [
    "## Choose the scenarioIndex (value between 1 and 4)\n",
    "scenarioIndex = 4\n",
    "##\n",
    "\n",
    "X0,X1,y = generate_scenario(scenario=scenarioIndex, n_samples0 = 350, n_samples1 = 350)\n",
    "X=np.concatenate((X0,X1),axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X0[:,0],X0[:,1],c='r', label='Class 0')\n",
    "plt.scatter(X1[:,0],X1[:,1],c='b', label='Class 1')\n",
    "plt.title('Original data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a60dmwTWEeDw"
   },
   "source": [
    "Use the K-means to find the different clusters in the dataset. K-means will be used on X and we will check whether it can well separate the two classes. \n",
    "\n",
    "\n",
    "**Question:**\n",
    "1. Does it work well in all scenarios ? Why ? Is it always easy to define the correct number of clusters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "bRxKO8tZEeDx",
    "outputId": "72c923d0-899e-44d2-f9b0-0e96bd86e2b2"
   },
   "outputs": [],
   "source": [
    "## K-means\n",
    "Ncluster= 2 # choose a number of clusters\n",
    "kmeans=KMeans(n_clusters=Ncluster) \n",
    "yKmeans=kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(17, 6))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[np.where(y == 0)[0],0],X[np.where(y == 0)[0],1],c='r', label='Class 0')\n",
    "plt.scatter(X[np.where(y == 1)[0],0],X[np.where(y == 1)[0],1],c='b', label='Class 1')\n",
    "plt.title('Original data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
    "plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1')\n",
    "plt.title('K-Means')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "frontiere(kmeans, X, y, step=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement your own K-means using the Lloyd's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansLecture(X,K=2,tol=1e-4):\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    #...\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    XXXXXXXX\n",
    "    \n",
    "    \n",
    "    return yKmeans,muTab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-means\n",
    "Ncluster= 2 # choose a number of clusters\n",
    "yKmeans,muTab = KMeansLecture(X,K=Ncluster,tol=1e-4)\n",
    "\n",
    "plt.figure(figsize=(17, 6))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[np.where(y == 0)[0],0],X[np.where(y == 0)[0],1],c='r', label='Class 0')\n",
    "plt.scatter(X[np.where(y == 1)[0],0],X[np.where(y == 1)[0],1],c='b', label='Class 1')\n",
    "plt.title('Original data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[np.where(yKmeans == 0)[0],0],X[np.where(yKmeans == 0)[0],1],c='r', label='Predicted class 0')\n",
    "plt.scatter(X[np.where(yKmeans == 1)[0],0],X[np.where(yKmeans == 1)[0],1],c='b', label='Predicted class 1')\n",
    "plt.title('K-Means')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-2) K-means for skin lesion segmentation\n",
    "\n",
    "\n",
    "In this section, you will use the K-means to segment skin lesion images. You will use two images from the ISIC dataset (www.isic-archive.com), one nevus and one melanoma with their respetive manual segmentation. \n",
    "\n",
    "**Goal**\n",
    "\n",
    "The goal of this section is to delineate the contours (i.e. segment) of the skin lesions using k-means. \n",
    "\n",
    "Let's first load the data. The images are rescaled so that the computations are faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  download_file_from_google_drive(file_id='1_TeYzLLDoKbPX4xXAOAM_mQiT2nLHgvp',dest_path='./data/nevus.jpg')\n",
    "  download_file_from_google_drive(file_id='1iQZdUiuK_FwZ7mik7LB3eN_H_IUc5l7b',dest_path='./data/nevus-seg.jpg')\n",
    "  download_file_from_google_drive(file_id='1yZ46UzGhwO7g5T8397JpewBl6UqgRo5J',dest_path='./data/melanoma.jpg')\n",
    "  download_file_from_google_drive(file_id='1B2Ol92mBcHN6ah3bpoucBbBbHkPMGC8D',dest_path='./data/melanoma-seg.png')\n",
    "else:\n",
    "  print('You are not using Colab. Please define working_dir with the absolute path to the folder where you downloaded the data')\n",
    "\n",
    "# Please modify working_dir only if you are using your Anaconda (and not Google Colab)\n",
    "Working_directory=\"./data/\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nevus\n",
    "nevus = imread(Working_directory + 'nevus.jpg')\n",
    "nevus=nevus[2:-2,2:-2,:] # remove border (it contains artifacts)\n",
    "nevusMask = imread(Working_directory + 'nevus-seg.jpg')\n",
    "nevusMask=nevusMask[2:-2,2:-2] # remove border (it contains artifacts)\n",
    "# We rescale to speed up computations\n",
    "nevus = rescale(nevus, 0.25, channel_axis = 2,anti_aliasing=True)\n",
    "# We need all these options to preserve the binary values\n",
    "nevusMask = rescale(nevusMask, 0.25, anti_aliasing=False, order=0,  preserve_range=True)\n",
    "nevusMask_boolean = (nevusMask/255).astype(np.uint8) # To get uint8\n",
    "nevusMask_expand = np.expand_dims(nevusMask_boolean, axis=2) # To have a 3 channels boolean mask\n",
    "\n",
    "# Melanoma\n",
    "melanoma = imread(Working_directory + 'melanoma.jpg')\n",
    "melanoma=melanoma[2:-2,2:-2,:] # remove border (it contains artifacts)\n",
    "melanomaMask = imread(Working_directory + 'melanoma-seg.png')\n",
    "melanomaMask=melanomaMask[2:-2,2:-2] # remove border (it contains artifacts)\n",
    "melanoma = rescale(melanoma, 0.25, channel_axis = 2,anti_aliasing=True)\n",
    "melanomaMask = rescale(melanomaMask, 0.25, anti_aliasing=False, order=0,  preserve_range=True)\n",
    "melanomaMask_boolean = (melanomaMask/255).astype(np.uint8) \n",
    "melanomaMask_expand = np.expand_dims(melanomaMask_boolean, axis=2)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols = (2, 3),\n",
    "                axes_pad = 0.5)\n",
    "grid[0].imshow(nevus)\n",
    "grid[0].set_title('Nevus')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(nevusMask_boolean,cmap='gray')\n",
    "grid[1].set_title('Nevus segmentation mask')\n",
    "grid[1].axis('off')\n",
    "grid[2].imshow(nevusMask_expand*nevus)\n",
    "grid[2].set_title('Nevus with segmentation')\n",
    "grid[2].axis('off')\n",
    "grid[3].imshow(melanoma)\n",
    "grid[3].set_title('Melanoma')\n",
    "grid[3].axis('off')\n",
    "grid[4].imshow(melanomaMask_boolean,cmap='gray')\n",
    "grid[4].set_title('Melanoma segmentation mask')\n",
    "grid[4].axis('off')\n",
    "grid[5].imshow(melanomaMask_expand*melanoma)\n",
    "grid[5].set_title('Melanoma with segmentation')\n",
    "grid[5].axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. Before running K-means, please answer this question. How many classes $K$ should you look for ? Would you use the same $K$ for both images ? Why ?\n",
    "\n",
    "2. Run the following code for both images. Try to choose different channels (among the channels Red, Green and Blue) and different number of clusters $K$. Which is the best choice in terms of channel and number of classes ? You can use the [Dice score](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to quantitatively compare your mask and the manual segmentation. Comment the results with respect to the previous answer.\n",
    "\n",
    "The dice score (or dice similarity) between two binary masks is defined as $\\frac{2TP}{2TP + FP + FN}$ and it ranges between 0 (completely different) and 1 (perfectly equal).\n",
    "\n",
    "3. In K-means, we recompute the average at each iteration. The average is not constrained to be one of the original observations. It is usually an *interpolation* of the original observations. How would you change the Lloyd's algorithm to constrain the average to always be one of the original observations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEVUS\n",
    "# Select a channel (0 for Red, 1 for Green and 2 for Blue)\n",
    "channel=2\n",
    "##\n",
    "nevusB = nevus[:,:,channel] \n",
    "\n",
    "# Select the number of cluster K to look for \n",
    "K= 2 # choose a number of clusters\n",
    "##\n",
    "kmeans=KMeans(n_clusters=K, random_state=1) \n",
    "labels=kmeans.fit_predict(nevusB.reshape(-1,1))\n",
    "labels=np.reshape(labels,(nevusB.shape[0],nevusB.shape[1]))\n",
    "\n",
    "# Depending on the number of classes K, K-means returns one integer per pixel \n",
    "# which indicates the number of the cluster. \n",
    "# Choose the integer to use as mask between 0 and K-1\n",
    "index =1\n",
    "mask=labels==index ## choose which label should be\n",
    "\n",
    "contourMask = find_contours(mask, 0.5)\n",
    "contourManual = find_contours(nevusMask_boolean, 0.5)\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols = (1, 5),\n",
    "                axes_pad = 0.5)\n",
    "grid[0].imshow(nevusB,cmap='gray')\n",
    "grid[0].set_title('Nevus - selected channel')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(labels,cmap='coolwarm')\n",
    "grid[1].set_title('Nevus K-means result')\n",
    "grid[1].axis('off')\n",
    "grid[2].imshow(mask*nevusB,cmap='gray')\n",
    "grid[2].set_title('Nevus with\\n chosen segmentation')\n",
    "grid[2].axis('off')\n",
    "grid[3].imshow(nevusMask_boolean*nevusB,cmap='gray')\n",
    "grid[3].set_title('Nevus with\\n manual segmentation')\n",
    "grid[3].axis('off')\n",
    "grid[4].imshow(nevus)\n",
    "for contour in contourMask:\n",
    "  grid[4].plot(contour[:, 1], contour[:, 0], linewidth=2, c='r')\n",
    "for contour in contourManual:\n",
    "  grid[4].plot(contour[:, 1], contour[:, 0], linewidth=2, c='g')\n",
    "grid[4].set_title('Green manual\\n Red K-means')\n",
    "grid[4].axis('off')\n",
    "\n",
    "# Compute the dice score between your mask and the manual segmentation \n",
    "print('The dice score is ', 1-dice(np.squeeze(nevusMask_boolean.reshape(1,-1)), np.squeeze(mask.reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MELANOMA\n",
    "# Select a channel (0 for Red, 1 for Green and 2 for Blue)\n",
    "channel=0\n",
    "##\n",
    "melanomaB = melanoma[:,:,channel]\n",
    "\n",
    "# Select the number of cluster K to look for \n",
    "K= 2 # choose a number of clusters\n",
    "##\n",
    "kmeans=KMeans(n_clusters=K) \n",
    "labels=kmeans.fit_predict(melanomaB.reshape(-1,1))\n",
    "labels=np.reshape(labels,(melanomaB.shape[0],melanomaB.shape[1]))\n",
    "\n",
    "# Depending on the number of classes K, K-means returns one integer per pixel \n",
    "# which indicates the number of the cluster. \n",
    "# Choose the integer to use as mask between 0 and K-1\n",
    "index =1\n",
    "mask=labels==index ## choose which label should be\n",
    "\n",
    "contourMask = find_contours(mask, 0.5)\n",
    "contourManual = find_contours(melanomaMask_boolean, 0.5)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols = (1, 5),\n",
    "                axes_pad = 0.5)\n",
    "grid[0].imshow(melanomaB,cmap='gray')\n",
    "grid[0].set_title('Melanoma - selected channel')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(labels,cmap='coolwarm')\n",
    "grid[1].set_title('Melanoma K-means result')\n",
    "grid[1].axis('off')\n",
    "grid[2].imshow(mask*melanomaB,cmap='gray')\n",
    "grid[2].set_title('Melanoma with chosen segmentation')\n",
    "grid[2].axis('off')\n",
    "grid[3].imshow(melanomaMask_boolean*melanomaB,cmap='gray')\n",
    "grid[3].set_title('Melanoma with\\n manual segmentation')\n",
    "grid[3].axis('off')\n",
    "grid[4].imshow(melanoma)\n",
    "for contour in contourMask:\n",
    "  grid[4].plot(contour[:, 1], contour[:, 0], linewidth=2, c='r')\n",
    "for contour in contourManual:\n",
    "  grid[4].plot(contour[:, 1], contour[:, 0], linewidth=2, c='g')\n",
    "grid[4].set_title('Green manual\\n Red K-means')\n",
    "grid[4].axis('off')\n",
    "\n",
    "print('The dice score is ', 1-dice(np.squeeze(melanomaMask_boolean.reshape(1,-1)), np.squeeze(mask.reshape(1,-1))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP_Unsupervised_0_ToyExamples_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
